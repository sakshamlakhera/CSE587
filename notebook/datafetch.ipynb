{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ba4c456-2d2b-4fc0-ab35-2ab2170987d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import List, Optional, Dict\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f9d73-77f7-43e1-b1b4-6f12d151d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pandas\n",
    "# !pip3 install pyarrow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b49026-f3aa-43fc-9e82-0d682006de47",
   "metadata": {},
   "source": [
    "### Data Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e74cf3b4-aeda-4e89-9e1e-520688ec130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_nsduh_data(year: int) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fetches NSDUH data for a specified year from a remote source.\n",
    "\n",
    "    Args:\n",
    "        year (int): The year for which to fetch data.\n",
    "\n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: A pandas DataFrame with the data, or None if fetching failed.\n",
    "    \"\"\"\n",
    "    url_placeholder = \"https://www.datafiles.samhsa.gov/sites/default/files/field-uploads-protected/studies/NSDUH-{year}/NSDUH-{year}-datasets/NSDUH-{year}-DS0001/NSDUH-{year}-DS0001-bundles-with-study-info/NSDUH-{year}-DS0001-bndl-data-tsv.zip\"\n",
    "    \n",
    "    try:\n",
    "        url = url_placeholder.format(year=year)\n",
    "        df = pd.read_csv(url, compression='zip', sep='\\t', low_memory=False)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for year {year}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c1f3873-a5b6-420e-9277-9edaa98fa6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_parquet_exists(years: List[int], output_dir: str) -> Dict[int, bool]:\n",
    "    \"\"\"\n",
    "    Checks if Parquet files for the specified years already exist in the output directory.\n",
    "\n",
    "    Args:\n",
    "        years (List[int]): A list of years to check.\n",
    "        output_dir (str): The directory where Parquet files are saved.\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, bool]: A dictionary with years as keys and boolean values indicating \n",
    "                         whether the Parquet file for that year exists.\n",
    "    \"\"\"\n",
    "    existence_check = {}\n",
    "    \n",
    "    for year in years:\n",
    "        year_path = os.path.join(output_dir, f'year={year}')\n",
    "        existence_check[year] = os.path.exists(year_path)\n",
    "    \n",
    "    return existence_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c5018e3-d52b-4661-b8ce-c26a8fc1cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet(df: pd.DataFrame, year: int, output_dir: str, overwrite: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Writes a DataFrame to Parquet format, partitioned by year.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to write.\n",
    "        year (int): The year of the data.\n",
    "        output_dir (str): The directory where Parquet files will be saved.\n",
    "        overwrite (bool): If True, overwrite existing files. If False, skip existing files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        year_dir = os.path.join(output_dir, f'year={year}')\n",
    "        if overwrite and os.path.exists(year_dir):\n",
    "            shutil.rmtree(year_dir)  # Remove existing directory to start fresh\n",
    "\n",
    "        df['year'] = year  # Add the year column for partitioning\n",
    "        # Write data to Parquet format with partitioning\n",
    "        df.to_parquet(output_dir, partition_cols=['year'], index=False)\n",
    "        \n",
    "        print(f\"Data for year {year} successfully saved to Parquet format in {output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to Parquet for year {year}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51986590-78ad-4ed7-9b4c-68672905ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_fetch(years_to_fetch: List[int], output_dir: str, overwrite: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Fetches NSDUH data for specified years and saves it to Parquet format, one year at a time.\n",
    "\n",
    "    Args:\n",
    "        years_to_fetch (List[int]): A list of years for which to fetch data.\n",
    "        output_dir (str): The directory where Parquet files will be saved.\n",
    "        overwrite (bool): If True, overwrite existing files. If False, skip existing files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for year in years_to_fetch:\n",
    "            if not overwrite and os.path.exists(os.path.join(output_dir, f'year={year}')):\n",
    "                print(f\"Data for year {year} already exists. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            df = fetch_nsduh_data(year)\n",
    "            if df is not None:\n",
    "                print(f\"Successfully fetched data for year: {year}\")\n",
    "                write_parquet(df, year, output_dir, overwrite)\n",
    "                del df  # Remove the DataFrame from memory\n",
    "                gc.collect()  # Force garbage collection\n",
    "\n",
    "        print(\"All requested years processed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred in the data_fetch function: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9af61774-cc8a-46d3-8acf-849d7e49e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet(input_dir: str, years: Optional[List[int]] = None) -> Dict[int, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Reads Parquet files for specified years from the input directory.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): The directory where Parquet files are stored.\n",
    "        years (Optional[List[int]]): A list of years to read. If None, read all available years.\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, pd.DataFrame]: A dictionary with years as keys and pandas DataFrames as values.\n",
    "    \"\"\"\n",
    "    data_frames = {}\n",
    "    available_years = [int(d.split('=')[1]) for d in os.listdir(input_dir) if d.startswith('year=')]\n",
    "    years_to_read = years if years is not None else available_years\n",
    "\n",
    "    for year in years_to_read:\n",
    "        year_path = os.path.join(input_dir, f'year={year}')\n",
    "        if os.path.exists(year_path):\n",
    "            df = pd.read_parquet(year_path)\n",
    "            data_frames[year] = df\n",
    "        else:\n",
    "            print(f\"Warning: No data found for year {year}\")\n",
    "\n",
    "    return data_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33055038-3cc7-4486-af8d-808f0559811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    years = [2015, 2016, 2017, 2018, 2019] \n",
    "    output_directory = \"../data/DS/NSDUH\"\n",
    "    \n",
    "    # Fetch and save data\n",
    "    data_fetch(years, output_directory, overwrite=False)\n",
    "\n",
    "    # Read saved data (if needed)\n",
    "    # Note: This part is optional and can be removed if you don't need to read the data immediately after saving\n",
    "    for year in years:\n",
    "        df = read_parquet(output_directory, [year])\n",
    "        if year in df:\n",
    "            print(f\"Data for year {year}:\")\n",
    "            print(df[year].head())\n",
    "        del df\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece1f9c-9d52-4a4c-9ef3-03fbc6268aa9",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f94f3-de41-4773-8aaa-bc8c6cbbd7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a46504-1236-4fbe-bda1-8a2f516bce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd78d40c-45b4-4d53-9ea3-b45c929f3e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068faa7-0ea6-437a-9477-1856cc52107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7574407-ccbf-4042-aa36-644f583dd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(100).to_csv(r\"../data/sample_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de81d2",
   "metadata": {},
   "source": [
    "EDA done by Apurva Umredkar (50592382) using a new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814d9b2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
